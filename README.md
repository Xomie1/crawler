# Web Crawler (Batch & Excel Driven)

A **production-ready, Excel-driven web crawler** for extracting company contact information at scale.  
Designed for **high-volume batch crawling**, with optional **AI-assisted extraction** for improved accuracy on difficult or non-standard websites.

**Status**: âœ… **Stable & Actively Used** â€“ Supports large batch runs and AI-powered enrichment

---

## ğŸ“‹ Features

- âœ… **Batch Crawling from Excel**
  - Process hundreds or thousands of company websites from a single Excel file
- âœ… **Email Extraction**
  - Detects, normalizes, and validates email addresses
- âœ… **Contact / Inquiry Form Detection**
  - Supports English & Japanese websites
  - Detects form URLs and embedded forms
- âœ… **Company Name Extraction**
  - Metadata, page structure, and AI-enhanced detection
- âœ… **Industry Detection**
  - Rule-based + AI-assisted extraction
- âœ… **AI-Assisted Extraction (Optional)**
  - Improves results for hard-to-parse or poorly structured sites
- âœ… **Robots.txt Compliance**
  - Respects crawling permissions by default
- âœ… **Retry & Error Handling**
  - Graceful degradation with detailed crawl status
- âœ… **Multiple Export Options**
  - Excel output
  - Google Sheets export
  - Google Apps Script integration
- âœ… **Scalable Batch Mode**
  - Supports large datasets with optional row limits for testing

---

## ğŸ“ Project Structure

```text
crawler-main/
â”œâ”€â”€ batch/ # Batch & CLI execution
â”‚ â”œâ”€â”€ batch_crawler.py # Main CLI entry point
â”‚ â”œâ”€â”€ excel_export.py # Excel output handling
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ ai_config.py # AI configuration & settings
â”‚
â”œâ”€â”€ crawler/
â”‚ â”œâ”€â”€ engine.py # Core crawling engine
â”‚ â”œâ”€â”€ fetcher.py # HTTP fetching & retries
â”‚ â”œâ”€â”€ parser.py # HTML parsing utilities
â”‚ â”œâ”€â”€ robots.py # Robots.txt handling
â”‚ â”œâ”€â”€ storage.py # Result storage & formatting
â”‚ â”‚
â”‚ â”œâ”€â”€ extractors/ # Rule-based extractors
â”‚ â”‚ â”œâ”€â”€ email_extractor.py
â”‚ â”‚ â”œâ”€â”€ enhanced_company_name_extractor.py
â”‚ â”‚ â”œâ”€â”€ enhanced_contact_form_detector.py
â”‚ â”‚ â”œâ”€â”€ improved_ai_company_extractor.py
â”‚ â”‚ â””â”€â”€ industry_extractor.py
â”‚ â”‚
â”‚ â””â”€â”€ ai/ # AI & hybrid extraction logic
â”‚ â”œâ”€â”€ ai_extractor.py
â”‚ â””â”€â”€ hybrid_extractor.py
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ logger.py # Logging utilities
â”‚ â”œâ”€â”€ groq_normalizer.py # AI response normalization
â”‚ â””â”€â”€ prompt_templates.py # AI prompt templates
â”‚
â”œâ”€â”€ load_env.py # Environment variable loader
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ test data.xlsx # Sample input
â””â”€â”€ README.md
```
---

## ğŸš€ Quick Start

### 1. Installation

```bash
pip install -r requirements.txt
Requires Python 3.10+ (tested on Python 3.11)
```

2. Basic Batch Crawl (No AI)
```bash
python batch/batch_crawler.py "excelfile.xlsx"
```
Crawls each website listed in the Excel file
Uses rule-based extractors only
Fast and cost-effective

3. Batch Crawl with AI Assistance
```bash
python batch/batch_crawler.py "excelfile.xlsx" --use-ai --ai-always
```
Enables AI extraction for:
Company name
Industry
Contact form detection (fallback)

Recommended for:
- Japanese sites
- Low-quality HTML
- JS-heavy pages

4. Small Batch / Test Run
```bash
python batch/batch_crawler.py "excelfile.xlsx" --use-ai --ai-always --limit n
```
Example:
```
python batch/batch_crawler.py "excelfile.xlsx" --use-ai --ai-always --limit 20
```
### ğŸ“Š Input Format (Excel)
Your Excel file should contain at least:
- Website URL column (root domain per company)
- All other columns are preserved and enriched with crawl results.

### ğŸ“¤ Output Data
Each row is augmented with structured crawl results:
```text
json
{
  "url": "https://example.com",
  "email": "info@example.com",
  "inquiryFormUrl": "https://example.com/contact",
  "companyName": "Example Co., Ltd.",
  "industry": "Manufacturing",
  "httpStatus": 200,
  "robotsAllowed": true,
  "crawlStatus": "success",
  "errorMessage": null
}
```
### ğŸ§  AI vs Non-AI Mode
| Mode            | Description         | When to Use               |
| --------------- | ------------------- | ------------------------- |
| Rule-based only | Deterministic, fast | Clean HTML, Western sites |
| Hybrid AI       | Rules + AI fallback | Mixed-quality sites       |
| AI Always       | AI-first extraction | Japanese / complex sites  |


AI behavior is configured in:
```
config/ai_config.py
```
### âš™ï¸ Core Components
- Crawler Engine: crawler/engine.py
- Coordinates fetching, parsing, and extraction
- Controls rule-based and AI-assisted workflows

### Extractors
***crawler/extractors/***
- Modular, reusable rule-based detectors
- Easy to extend

## AI Layer
***crawler/ai/***
- Prompt-based extraction
- Hybrid logic merges deterministic + AI results

### Batch Runner
**batch/batch_crawler.py**
- CLI entry point
- Excel input/output
- Supports row limits and AI flags

### ğŸ“ Logging
Centralized logging via:
```
from utils.logger import setup_logger
```
- INFO-level by default
- Crawl failures never halt the batch

### ğŸ”’ Crawling Behavior & Safety
- Respects robots.txt by default
- One root URL crawl per company
- No deep crawling (intentional for scale)
- Safe retry and timeout handling

### âš¡ Performance Notes
- Optimized for large Excel batches
- Robots.txt cached per domain
- Suitable for 10,000+ rows depending on network and AI usage

### Form Detection
The crawler detects inquiry/contact forms using:

- Form tags with inquiry-related keywords
- Button labels (English and Japanese supported)
- Link text containing form-related keywords

Supported keywords include:
- English: "contact", "inquiry", "consultation", "form", etc.
- Japanese: "å•ã„åˆã‚ã›", "ãŠå•ã„åˆã‚ã›", "ç›¸è«‡", etc.

## Error Handling
The crawler implements comprehensive error handling:

- Network errors: Automatic retry with exponential backoff
- Timeout errors: Configurable timeout with retry
- Parsing errors: Graceful degradation with error logging
- Robots.txt errors: Defaults to allowing crawl if robots.txt is inaccessible

ğŸ“„ License
Provided as-is for internal automation, research, and data enrichment workflows.
